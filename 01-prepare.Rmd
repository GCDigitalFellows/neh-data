---
title: "Prepare data"
---

```{r echo=FALSE, warning=FALSE, message=FALSE, results='hide'}
source('.Rprofile')
library(tidyverse)
library(rvest)
library(xml2)
library(jsonlite)
library(forcats)
library(ggmap)
library(broom)
library(noncensus)
library(blscrapeR)
library(htmltools)
library(knitr)
data("states")
theme_set(theme_bw())
knitr::opts_chunk$set(fig.width=12, fig.height=8,
                      warning=FALSE, message=FALSE)
```

### Downloading source files

Fortunately, the NEH provides open access to their [grants data](https://securegrants.neh.gov/open/data/). We first use `rvest::read_html` to get a list of all the zipped grant files:

```{r}
listnehgrantzips <- function() {
  zipurls <- read_html('https://securegrants.neh.gov/open/data') %>%
    html_nodes('a[href*="NEH_Grants"][href$=".zip"]') %>%
    html_attr('href') %>%
    paste0('https://securegrants.neh.gov/', .)
}
```

Each grant zip file contains an XML data file and an XML Schema Definition file, which we can ignore for our current purposes. We save the XML to a directory in our project directory:

```{r}
downloadxml <- function(url) {
  DATADIR <- './.data'
  # intermediate files will be saved between steps
  dir.create(DATADIR, showWarnings = FALSE)
  xmlfile <- gsub(".zip", ".xml", basename(url))
  # only download if xml file doesn't already exist
  if(!file.exists(file.path(DATADIR, xmlfile))) {
    tf <- tempfile()
    download.file(url, tf, quiet = TRUE)
    unzip(tf, xmlfile, exdir=DATADIR)
  } else {
    file.path(DATADIR, xmlfile)
  }
}
```

Finally, we pull these two functions together using `purrr::map` to download each file:

```{r}
xmlfiles <- listnehgrantzips() %>% map(~ downloadxml(.)) %>% unlist
```

### Extracting data from source files

The XML format can be difficult to load into a data table for analysis. It requires us to transform its nested structure into a unnested table form, with rows and columns. We use the `xml2::read_xml` function to pull nodes out of the XML tree structure:

```{r}
read_xml(tail(xmlfiles, 1)) %>% 
  xml_find_all('./Grant') %>%
  head(1) %>%
  xml_children()
```

We need to pull out the `AppNumber` attribute since this is the unique number used by the NEH to identify awarded grants.

Of the 28 fields that are in the nodeset for each Grant, `Participant` and `Discipline` are nested nodes. The `Discipline` node is simply a list of `Name` nodes.

```{r}
read_xml('.data/NEH_Grants2000s.xml') %>% 
  xml_find_all('./Grant/*[count(*) > 0]') %>%
  head(5)
```

Following [How to tame XML with nested data frames and purrr](https://github.com/jennybc/manipulate-xml-with-purrr-dplyr-tidyr#readme), we create the following function to transform our XML data to a data frame:

```{r}
xmltodf <- function(xmlfile) {
  grants <- read_xml(xmlfile) %>% xml_find_all('./Grant')
  df <- data_frame(row = seq_along(grants),
                   nodeset = grants) %>%
    mutate(col_name_raw = nodeset %>% map(~ xml_children(.)) %>% map(~ xml_name(.)),
           cell_text = nodeset %>% map(~ xml_children(.)) %>% map(~ xml_text(.)),
           appnumber = nodeset %>% xml_attr('AppNumber'),
           i = nodeset %>% map(~ xml_children(.)) %>% map(~ seq_along(.))) %>%
    select(row, i, appnumber, col_name_raw, cell_text) %>%
    unnest() %>%
    group_by(row, appnumber, col_name_raw) %>% 
    summarise(cell_text = toString(cell_text)) %>%
    ungroup() %>%
    spread(col_name_raw, cell_text)
  df
}
```

We use the `xmltodf` function to through all the XML files we have downloaded and transform them into data frames that we finally combine into one, which we save to the file system to avoid redoing the same steps as we continue our exploration of the data.

```{r}
preparegrantsdata <- function() {
  grants <- xmlfiles %>% 
    map(~ xmltodf(.)) %>%
    bind_rows() %>%
    mutate_if(is.list, as.character) %>%
    mutate_if(is.character, funs(type.convert)) %>%
    select(-ProjectDesc, -ToSupport) %>% # FIXME: These fields are causing duplicates
    distinct()
}
if(!file.exists('.RData')) {
  grants <- preparegrantsdata()
  save.image()
} else {
  load('.RData')
}
```

For mapping purposes, we can geocode the zip codes provided in the dataset. Since we are `taRifx.geo::geocode` with the Bing mapping service, you will need to add the Bing Map API key.

```{r warning=FALSE, message=FALSE, results='hide'}
geolocations <- grants %>%
  distinct(InstPostalCode) %>%
  rowwise() %>% 
  mutate(LatLong = toString(taRifx.geo::geocode(InstPostalCode, service='bing'))) %>%
  separate(LatLong, c('Lat', 'Long'), sep=',')
```

### Caveats

In defining NEH funding, the [NEH Grants Data Dictionary](https://securegrants.neh.gov/Open/data/NEH_GrantsDictionary.pdf) explains that there are five different dollar amounts included in the grants dataset:

1. ApprovedOutright: Approved amount (outright funds). (Outright funds are not contingent on additional fundraising.) 
2. ApprovedMatching: Approved amount (matching funds). (Federal matching funds require a grantee to secure gift funds from third parties before federal funds are awarded. Except for Challenge Grants, NEH matching awards are made on a one-to-one basis.) 
3. AwardOutright: Amount actually awarded (outright funds). 
4. AwardMatching: Amount actually awarded (matching funds).
5. OriginalAmount: Original amount of grant (minus any grant supplements).

We will define the relevant `funding` value as the sum of `ApprovedOutright` and `ApprovedMatching`. 

Furthermore, we will adjust funding dollars into 2016 dollars using the `blscrapeR::inflation_adjust` function:

```{r warning=FALSE, message=FALSE, results='hide'}
cpiadj <- inflation_adjust(2017) %>%
  data_frame(YearAwarded = seq_along(.) + 1946, 
             Adj = .)
```